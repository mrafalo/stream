{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58812ac2-282b-41e9-a4e8-60d2d68887c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-28 09:03:17 total events sent: 10 last 20 seconds: 10 running: 1\n",
      "2022-08-28 09:03:37 total events sent: 20 last 20 seconds: 10 running: 1\n",
      "2022-08-28 09:03:58 total events sent: 30 last 20 seconds: 10 running: 1\n",
      "2022-08-28 09:04:18 total events sent: 40 last 20 seconds: 10 running: 1\n",
      "2022-08-28 09:04:38 total events sent: 50 last 20 seconds: 10 running: 1\n",
      "2022-08-28 09:04:59 total events sent: 60 last 20 seconds: 10 running: 1\n",
      "2022-08-28 09:05:19 total events sent: 70 last 20 seconds: 10 running: 1\n",
      "2022-08-28 09:05:39 total events sent: 80 last 20 seconds: 10 running: 1\n",
      "2022-08-28 09:05:59 total events sent: 90 last 20 seconds: 10 running: 1\n",
      "2022-08-28 09:06:20 total events sent: 100 last 20 seconds: 10 running: 1\n",
      "2022-08-28 09:06:40 total events sent: 110 last 20 seconds: 10 running: 1\n",
      "2022-08-28 09:07:00 total events sent: 120 last 20 seconds: 10 running: 1\n",
      "2022-08-28 09:07:20 total events sent: 130 last 20 seconds: 10 running: 1\n",
      "2022-08-28 09:07:41 total events sent: 140 last 20 seconds: 10 running: 1\n",
      "2022-08-28 09:08:01 total events sent: 150 last 20 seconds: 10 running: 1\n",
      "2022-08-28 09:08:21 total events sent: 160 last 20 seconds: 10 running: 1\n",
      "2022-08-28 09:08:41 total events sent: 170 last 20 seconds: 10 running: 1\n",
      "2022-08-28 09:09:02 total events sent: 180 last 20 seconds: 10 running: 1\n",
      "2022-08-28 09:09:22 total events sent: 190 last 20 seconds: 10 running: 1\n",
      "2022-08-28 09:09:42 total events sent: 200 last 20 seconds: 10 running: 1\n",
      "2022-08-28 09:10:02 total events sent: 210 last 20 seconds: 10 running: 1\n",
      "2022-08-28 09:10:23 total events sent: 220 last 20 seconds: 10 running: 1\n",
      "2022-08-28 09:10:43 total events sent: 230 last 20 seconds: 10 running: 1\n",
      "2022-08-28 09:11:03 total events sent: 240 last 20 seconds: 10 running: 1\n",
      "2022-08-28 09:11:23 total events sent: 422 last 20 seconds: 182 running: 1\n",
      "2022-08-28 09:11:43 total events sent: 604 last 20 seconds: 182 running: 1\n",
      "2022-08-28 09:12:03 total events sent: 786 last 20 seconds: 182 running: 1\n",
      "2022-08-28 09:12:23 total events sent: 968 last 20 seconds: 182 running: 1\n",
      "2022-08-28 09:12:43 total events sent: 1150 last 20 seconds: 182 running: 1\n",
      "2022-08-28 09:13:04 total events sent: 1332 last 20 seconds: 182 running: 1\n",
      "2022-08-28 09:13:24 total events sent: 1514 last 20 seconds: 182 running: 1\n",
      "2022-08-28 09:13:44 total events sent: 1696 last 20 seconds: 182 running: 1\n",
      "2022-08-28 09:14:04 total events sent: 1878 last 20 seconds: 182 running: 1\n",
      "2022-08-28 09:14:24 total events sent: 4218 last 20 seconds: 2340 running: 1\n",
      "2022-08-28 09:14:44 total events sent: 6554 last 20 seconds: 2336 running: 1\n",
      "2022-08-28 09:15:04 total events sent: 8901 last 20 seconds: 2347 running: 1\n",
      "Maximum number of events reached: 10000\n",
      "Maximum number of events reached: 10000\n"
     ]
    }
   ],
   "source": [
    "import sys, csv\n",
    "import time\n",
    "import json\n",
    "from kafka import KafkaProducer\n",
    "import datetime\n",
    "import yaml\n",
    "\n",
    "with open(r'config.yaml') as file:\n",
    "    cfg = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "    KAFKA_SERVER = cfg['global']['bootstrap_server']\n",
    "    DATA_FILE = cfg['global']['data_file']\n",
    "    KAFKA_TOPIC = cfg['global']['topic']\n",
    "    LINGER_MS = cfg['producer']['linger_ms']\n",
    "    DELAY_SEC = cfg['global']['delay_sec']\n",
    "    BATCH_SIZE = cfg['producer']['batch_size']\n",
    "    RECORD_CNT = cfg['global']['record_cnt']\n",
    "    RUN_FLAG = cfg['global']['running']\n",
    "\n",
    "def error(exception):\n",
    "    print(exception)\n",
    "\n",
    "def kafka_python_producer_async(_producer, _msg, _topic):\n",
    "    _producer.send(topic = _topic, value = _msg).add_errback(error)\n",
    "    _producer.flush()\n",
    "    \n",
    "cnt = 0\n",
    "cnt_window = 0\n",
    "producer = KafkaProducer(bootstrap_servers = KAFKA_SERVER, linger_ms = LINGER_MS)\n",
    "exp_time = datetime.datetime.now().timestamp()\n",
    "\n",
    "WINDOW_INTERVAL = 20\n",
    "\n",
    "while True:\n",
    "    data = csv.reader(open(DATA_FILE), delimiter=\",\")\n",
    "    colnames = ['actor_login', 'actor_id', 'comment_id', 'comment', 'repo', 'language', \n",
    "                'author_login', 'author_id', 'pr_id', 'c_id', 'commit_date', \n",
    "                'topic', 'delay_sec', 'batch_size', 'linger_ms', 'timestamp']\n",
    "\n",
    "    next(data, None)  # skip the headers\n",
    "  \n",
    "    for r in data:\n",
    "        \n",
    "        time_diff = datetime.datetime.now().timestamp() - exp_time\n",
    "        \n",
    "        \n",
    "        if time_diff>WINDOW_INTERVAL:\n",
    "            print(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"total events sent:\", cnt, \"last\", WINDOW_INTERVAL, \"seconds:\", cnt_window, \"running:\", RUN_FLAG)\n",
    "            cnt_window = 0\n",
    "            exp_time = datetime.datetime.now().timestamp()\n",
    "            \n",
    "            with open(r'config.yaml') as file:\n",
    "                cfg = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "                KAFKA_SERVER = cfg['global']['bootstrap_server']\n",
    "                DATA_FILE = cfg['global']['data_file']\n",
    "                KAFKA_TOPIC = cfg['global']['topic']\n",
    "                LINGER_MS = cfg['producer']['linger_ms']\n",
    "                DELAY_SEC = cfg['global']['delay_sec']\n",
    "                BATCH_SIZE = cfg['producer']['batch_size']\n",
    "                RECORD_CNT = cfg['global']['record_cnt']\n",
    "                RUN_FLAG = cfg['global']['running']\n",
    "                \n",
    "            producer = KafkaProducer(bootstrap_servers = KAFKA_SERVER, linger_ms = LINGER_MS)\n",
    "            \n",
    "        \n",
    "        msg = r\n",
    "        msg.append(KAFKA_TOPIC)\n",
    "        msg.append(DELAY_SEC)\n",
    "        msg.append(BATCH_SIZE)\n",
    "        msg.append(LINGER_MS)    \n",
    "        msg.append(datetime.datetime.now().timestamp())    \n",
    "        msg_flat = ';'.join(str(m) for m in msg)\n",
    "        \n",
    "        if RUN_FLAG == 1:\n",
    "            kafka_python_producer_async(producer, msg_flat.encode(), KAFKA_TOPIC)\n",
    "            cnt = cnt + 1\n",
    "            cnt_window = cnt_window + 1\n",
    "        \n",
    "        time.sleep(DELAY_SEC)\n",
    "        \n",
    "        #print(msg_flat)\n",
    "\n",
    "        if (cnt > RECORD_CNT) and (RECORD_CNT > 0):\n",
    "            print(\"Maximum number of events reached:\", RECORD_CNT)\n",
    "            break\n",
    "        \n",
    "    if (cnt > RECORD_CNT) and (RECORD_CNT > 0):\n",
    "        print(\"Maximum number of events reached:\", RECORD_CNT)\n",
    "        break\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
