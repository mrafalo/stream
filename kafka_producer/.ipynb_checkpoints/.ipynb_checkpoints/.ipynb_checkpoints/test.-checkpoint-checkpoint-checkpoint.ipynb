{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58812ac2-282b-41e9-a4e8-60d2d68887c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ottaviano;2375489;234631519;It's to have a link style and the `:hover` behavior.;en-marche.fr;;Haitaar;6321502;50045779;1208535182;2019-01-07 14:26:18 UTC;spark_test;1;16000;0;1661155018.15791\n",
      "thomasbisignani;3778670;244176167;What about adding these two methods in the `EntityThreadCommentStatusTrait` ? and set directly the property like : \\\\```php \\public function approve(): void\\{\\    $this->status = ThreadCommentStatusEnum::APPROVED;\\}\\;en-marche.fr;;Haitaar;6321502;52353175;1251855507;2019-01-07 14:26:18 UTC;spark_test;1;16000;0;1661155019.167205\n",
      "MaEtUgR;8511268;279244099;Done in #11920;Firmware;C++;cmic0;48283638;60891588;1373780061;2019-01-07 09:20:31 UTC;spark_test;1;16000;0;1661155020.16865\n",
      "enzozccr;48729257;281114600;Good question ^^\\\\by the way, this code is not from me;en-marche.fr;;Haitaar;6321502;61829484;1386346659;2019-01-07 17:50:34 UTC;spark_test;1;16000;0;1661155021.170299\n",
      "miguelmaso;33907256;275717816;i've removed since i'm calling the find_nodal_h_process from the python script;Kratos;;frastellini;43777742;60360930;1365578766;2019-01-07 11:08:55 UTC;spark_test;1;16000;0;1661155022.171663\n",
      "bobsilverberg;75268;193525196;We tend to always use template string for this sort of thing (even though concatenation is briefer).;addons-frontend;;willdurand;858;40435486;1033425698;2019-01-07 13:47:44 UTC;spark_test;1;16000;0;1661155023.17324\n",
      "bobsilverberg;75268;192420749;Just to be explicit, do you want to also do an assert.notCalled, or is that overkill?;addons-frontend;;willdurand;858;40122136;1026325759;2019-01-07 09:22:34 UTC;spark_test;1;16000;0;1661155024.17461\n",
      "ESeiler;9540331;285455158;Yes, I also saw this, but it's copy/paste ... so I don't think it's necessarily within the scope of this PR (though it would be nice);seqan3;;MitraDarja;10737834;62652511;1398168495;2019-01-07 16:56:42 UTC;spark_test;1;16000;0;1661155025.176589\n",
      "mattrobenolt;28589;213162100;This relationship doesn't exist. So it did nothing.;sentry;Python;billyvg;62070;36825947;1109417280;2019-01-07 23:32:31 UTC;spark_test;1;16000;0;1661155026.178334\n",
      "houndci-bot;11568779;234221353;undefined name 'event';home-assistant;Python;fredrike;73288;49845843;1205637906;2019-01-07 12:04:53 UTC;spark_test;1;16000;0;1661155027.180092\n",
      "lauryndbrown;9348544;269757674;Hmm.. this is a bit unrelated but, should `send_to_list` and `teams_to_resolve` have been sets instead of lists? I was wondering why not just take the difference between `disabled_users` and `send_to_list`, but I see that's a list as the name suggests. J;sentry;Python;billyvg;62070;58644866;1341323258;2019-01-07 18:24:52 UTC;spark_test;1;16000;0;1661155028.181808\n",
      "Maximum number of record reached: %s 11\n"
     ]
    }
   ],
   "source": [
    "import sys, csv\n",
    "import time\n",
    "import json\n",
    "from kafka import KafkaProducer\n",
    "import datetime\n",
    "import yaml\n",
    "\n",
    "with open(r'config.yaml') as file:\n",
    "    cfg = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "    KAFKA_SERVER = cfg['global']['bootstrap_server']\n",
    "    DATA_FILE = cfg['global']['data_file']\n",
    "    KAFKA_TOPIC =cfg['global']['topic']\n",
    "    LINGER_MS =cfg['producer']['linger_ms']\n",
    "    DELAY_SEC =cfg['global']['delay_sec']\n",
    "    BATCH_SIZE =cfg['producer']['batch_size']\n",
    "    RECORD_CNT =cfg['global']['record_cnt']\n",
    "\n",
    "def success(metadata):\n",
    "    print(metadata.offset)\n",
    "\n",
    "def error(exception):\n",
    "    print(exception)\n",
    "\n",
    "def kafka_python_producer_async(_producer, _msg, _topic, _delay_sec):\n",
    "    #_producer.send(topic = _topic, value = _msg).add_callback(success).add_errback(error)\n",
    "    _producer.send(topic = _topic, value = _msg).add_errback(error)\n",
    "    time.sleep(_delay_sec)\n",
    "\n",
    "    _producer.flush()\n",
    "    \n",
    "\n",
    "\n",
    "cnt = 0\n",
    "producer = KafkaProducer(bootstrap_servers = KAFKA_SERVER, linger_ms = LINGER_MS)\n",
    "exp_time = datetime.datetime.now().timestamp()\n",
    "\n",
    "while True:\n",
    "    data = csv.reader(open(DATA_FILE), delimiter=\",\")\n",
    "    colnames = ['actor_login', 'actor_id', 'comment_id', 'comment', 'repo', 'language', \n",
    "                'author_login', 'author_id', 'pr_id', 'c_id', 'commit_date', \n",
    "                'topic', 'delay_sec', 'batch_size', 'linger_ms', 'timestamp']\n",
    "\n",
    "    next(data, None)  # skip the headers\n",
    "  \n",
    "    for r in data:\n",
    "        \n",
    "        time_diff = datetime.datetime.now().timestamp() - exp_time\n",
    "        \n",
    "        \n",
    "        if time_diff>20:\n",
    "            print(datetime.datetime.now())\n",
    "            exp_time = datetime.datetime.now().timestamp()\n",
    "            \n",
    "            with open(r'config.yaml') as file:\n",
    "                cfg = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "                KAFKA_SERVER = cfg['global']['bootstrap_server']\n",
    "                DATA_FILE = cfg['global']['data_file']\n",
    "                KAFKA_TOPIC =cfg['global']['topic']\n",
    "                LINGER_MS =cfg['producer']['linger_ms']\n",
    "                DELAY_SEC =cfg['global']['delay_sec']\n",
    "                BATCH_SIZE =cfg['producer']['batch_size']\n",
    "                \n",
    "            producer = KafkaProducer(bootstrap_servers = KAFKA_SERVER, linger_ms = LINGER_MS)\n",
    "            \n",
    "        cnt = cnt + 1\n",
    "        msg = r\n",
    "        msg.append(KAFKA_TOPIC)\n",
    "        msg.append(DELAY_SEC)\n",
    "        msg.append(BATCH_SIZE)\n",
    "        msg.append(LINGER_MS)    \n",
    "        msg.append(datetime.datetime.now().timestamp())    \n",
    "        msg_flat = ';'.join(str(m) for m in msg)\n",
    "        kafka_python_producer_async(producer, msg_flat.encode(), KAFKA_TOPIC, DELAY_SEC)\n",
    "\n",
    "        #print(msg_flat)\n",
    "\n",
    "        if (cnt > RECORD_CNT) and (RECORD_CNT > 0):\n",
    "            print(\"Maximum number of record reached: \", RECORD_CNT)\n",
    "            break\n",
    "        \n",
    "    if (cnt > RECORD_CNT) and (RECORD_CNT > 0):\n",
    "        break\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
